{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdgIuYwEQYRS"
      },
      "source": [
        "# Deep Learning for satellite image downloading\n",
        "\n",
        "Ali Ben Abbes; Jeaneth Machicao\n",
        "ali.benabbes@fondationbiodiversite.fr; machicao@usp.br\n",
        "\n",
        "Presented at VI WORKSHOP ON DATA SCIENCE AND MACHINE LEARNING-PARSEC, 03-06 October 2022, SÃ£o Paulo, Brazil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFFfGuyVpiLm"
      },
      "source": [
        "# PART 2: Google Earth Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOCxLueRdpK1"
      },
      "source": [
        "## Pre-requisites\n",
        "Register a Google account at [https://code.earthengine.google.com](https://code.earthengine.google.com). This process may take a couple of days. Without registration, the `ee.Initialize()` command below will throw an error message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5Ai5lP8zUGa",
        "outputId": "7aa739c3-4e56-4a96-c255-fed2d1c7440e"
      },
      "outputs": [],
      "source": [
        "!pip install earthengine-api --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Instructions\n",
        "\n",
        "This notebook exports Landsat satellite image composites clusters from Google Earth Engine.\n",
        "\n",
        "The images are saved in gzipped TFRecord format. By default, this notebook exports images to Google Drive. If you instead prefer to export images to Google Cloud Storage (GCS), change the `EXPORT` constant below to `'gcs'` and set `BUCKET` to the desired GCS bucket name.\n",
        "\n",
        "\n",
        "\n",
        "|      | Google Drive (default) | GCS\n",
        "|------|:-----------------------|:---\n",
        "| VR  | `dhs_tfrecords_raw/`   | `{BUCKET}/dhs_tfrecords_raw/`\n",
        "\n",
        "Once the images have finished exporting, download the exported TFRecord files to the following folder:\n",
        "\n",
        "- VR: `data/dhs_tfrecords_raw/` \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PVwwNTeSzjcK"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FibNHhq_QUwc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Any, Optional\n",
        "import ee\n",
        "import ee_utils\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuCpM5rbQgtP",
        "outputId": "26b5fc5d-3fa1-4d58-e662-877c50b01ced"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<p>To authorize access needed by Earth Engine, open the following\n",
              "        URL in a web browser and follow the instructions:</p>\n",
              "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=bZX3fQEguQSv3qXBd4iMqPbz7pX1b-mZDPUugIUUoCs&tc=b1v69YnQgPKE5Vy9WYQBx4AcxK7T-hFdpUGYXlX2H90&cc=y8-uFxvMhPTNVusAyxO3EZu678_z_ln_EC8mB26Nydo>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=bZX3fQEguQSv3qXBd4iMqPbz7pX1b-mZDPUugIUUoCs&tc=b1v69YnQgPKE5Vy9WYQBx4AcxK7T-hFdpUGYXlX2H90&cc=y8-uFxvMhPTNVusAyxO3EZu678_z_ln_EC8mB26Nydo</a></p>\n",
              "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully saved authorization token.\n"
          ]
        }
      ],
      "source": [
        "ee.Authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ISAyHZS8wBX5"
      },
      "outputs": [],
      "source": [
        "ee.Initialize()  # initialize the Earth Engine API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XkLHuR2zwBa4"
      },
      "outputs": [],
      "source": [
        "# ========== ADAPT THESE PARAMETERS ==========\n",
        "\n",
        "# To export to Google Drive, uncomment the next 2 lines\n",
        "#export: str, 'drive' for Google Drive, 'gcs' for GCS\n",
        "EXPORT = 'drive'\n",
        "BUCKET = None\n",
        "\n",
        "\n",
        "\n",
        "# export location paramet\n",
        "DHS_EXPORT_FOLDER = 'dhs_tfrecords_raw'\n",
        "DHSNL_EXPORT_FOLDER = 'dhsnl_tfrecords_raw'\n",
        "LSMS_EXPORT_FOLDER = 'lsms_tfrecords_raw'\n",
        "\n",
        "# Set CHUNK_SIZE to None to export a single TFRecord file per (country, year). However,\n",
        "# this may fail if it exceeds Google Earth Engine memory limits. Decrease CHUNK_SIZE\n",
        "# to a small number (<= 50) until Google Earth Engine stops reporting memory errors\n",
        "CHUNK_SIZE = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjCzT8UHwBde",
        "outputId": "c120b4c2-63f6-4613-ac98-c7d206bc10bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/machicao/Documents/WDS6_hands_on\n"
          ]
        }
      ],
      "source": [
        "# ========== DO NOT MODIFY THESE ==========\n",
        "\n",
        "# input data paths\n",
        "DHS_CSV_PATH = 'VR_clusters.csv' \n",
        "#add csv file\n",
        "\n",
        "\n",
        "# band names\n",
        "MS_BANDS = ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TEMP1']\n",
        "\n",
        "# image parameters\n",
        "PROJECTION = 'EPSG:3857'  # see https://epsg.io/3857\n",
        "SCALE = 30                # export resolution: 30m/px\n",
        "EXPORT_TILE_RADIUS = 127  # image dimension = (2*EXPORT_TILE_RADIUS) + 1 = 255px\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "suKVE0hxRTtS"
      },
      "outputs": [],
      "source": [
        "def export_images(df: pd.DataFrame,\n",
        "                  country: str,\n",
        "                  year: int,\n",
        "                  export_folder: str,\n",
        "                  chunk_size: Optional[int] = None\n",
        "                  ) -> dict[tuple[str, str, int, int], ee.batch.Task]:\n",
        "    '''\n",
        "    Args\n",
        "    - df: pd.DataFrame, contains columns ['lat', 'lon', 'country', 'year']\n",
        "    - country: str, together with `year` determines the survey to export\n",
        "    - year: int, together with `country` determines the survey to export\n",
        "    - export_folder: str, name of folder for export\n",
        "    - chunk_size: int, optionally set a limit to the # of images exported per TFRecord file\n",
        "        - set to a small number (<= 50) if Google Earth Engine reports memory errors\n",
        "\n",
        "    Returns: dict, maps task name tuple (export_folder, country, year, chunk) to ee.batch.Task\n",
        "    '''\n",
        "    subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
        "    if chunk_size is None:\n",
        "        chunk_size = len(subset_df)\n",
        "    num_chunks = int(math.ceil(len(subset_df) / chunk_size))\n",
        "    tasks = {}\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        chunk_slice = slice(i * chunk_size, (i+1) * chunk_size - 1)  # df.loc[] is inclusive\n",
        "        fc = ee_utils.df_to_fc(subset_df.loc[chunk_slice, :])\n",
        "        start_date, end_date = ee_utils.surveyyear_to_range(year)\n",
        "\n",
        "        # create 3-year Landsat composite image\n",
        "        roi = fc.geometry()\n",
        "        imgcol = ee_utils.LandsatSR(roi, start_date=start_date, end_date=end_date).merged\n",
        "        imgcol = imgcol.map(ee_utils.mask_qaclear).select(MS_BANDS)\n",
        "        img = imgcol.median()\n",
        "\n",
        "        # add nightlights, latitude, and longitude bands\n",
        "        img = ee_utils.add_latlon(img)\n",
        "        img = img.addBands(ee_utils.composite_nl(year))\n",
        "\n",
        "        fname = f'{country}_{year}_{i:02d}'\n",
        "        tasks[(export_folder, country, year, i)] = ee_utils.get_array_patches(\n",
        "            img=img, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
        "            points=fc, export=EXPORT,\n",
        "            prefix=export_folder, fname=fname,\n",
        "            bucket=BUCKET)\n",
        "    return tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JKqa5xrdwWw_"
      },
      "outputs": [],
      "source": [
        "tasks: dict[tuple[str, str, int, int], ee.batch.Task] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "n2EqpwNOwW6b",
        "outputId": "37657e2e-779b-45ff-c76c-ad88d7783d53"
      },
      "outputs": [],
      "source": [
        "dhs_df = pd.read_csv(DHS_CSV_PATH, float_precision='high', index_col=False, sep=';')\n",
        "dhs_surveys = list(dhs_df.groupby(['country', 'year']).groups.keys())\n",
        "\n",
        "for country, year in dhs_surveys:\n",
        "    new_tasks = export_images(\n",
        "        df=dhs_df, country=country, year=year,\n",
        "        export_folder=DHS_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
        "    tasks.update(new_tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RQBifmmGwW9D"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bf7d06a6e274a45b524c02e895994f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task ('dhs_tfrecords_raw', 'Brazil', 2010, 0) finished in 0 min with state: COMPLETED\n"
          ]
        }
      ],
      "source": [
        "ee_utils.wait_on_tasks(tasks, poll_interval=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c38f7b306aa25dc9c3eab171a46f32106c77906815498bbd230708409443bf6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
